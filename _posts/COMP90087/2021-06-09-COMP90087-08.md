---
layout:     post   				    # 使用的布局（不需要改）
title:      人工智能伦理 08：可解释性   	# 标题 
subtitle:   墨尔本大学 COMP90087 课程笔记 #副标题
date:       2021-06-09				# 时间
author:     YEY 						# 作者
header-img: img/post-bg-unimelb.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true                       # 是否启用 MathJax
tags:								#标签
    - 人工智能伦理
    - COMP90087
    - 课程笔记
---

# Module 08 可解释性

## 1. 动机：为什么要问为什么？

### 1.1 什么是可解释人工智能？

**可解释性（Explainability，以及 interpretability）**就是 *理解（understanding）*。

**可解释人工智能（Explainable AI）**是人们能够理解人工智能模型和决策的能力。

**解释（Explanation）**是一种帮助人们理解的机制。

### 1.2 为什么我们关心可解释性？

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-005101%402x.png" width="70%">

<center><font size=2>（来源：K. Stubbs et al.: Autonomy and Common Ground in Human-Robot Interaction: A Field Study, IEEE Intelligent Systems, 22(2):42-50, 2007.）</font></center>

### 1.3 可解释人工智能的目标

为什么我们关心可解释性？

* **信任：**合同中的正当信任和不信任
* **伦理：**通过产生信任来提高应用程序的伦理适用性

如果某些人无法理解一个算法生成某种决策的原因，那么让他们对基于该算法辅助的决策负责是否合理？

### 1.4 哪些人关心可解释性 AI？在什么阶段关注？

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-005749%402x.png" width="100%">

<center><font size=2>（来源：V. Belle and I. Papantonis: Principles and practice of explainable machine learning, arXiv, 2020. <https://arxiv.org/abs/2009.11698>）</font></center>

## 2. 可解释性 AI 的挑战

### 2.1 挑战：不透明度 (Opacity)

**决策树 vs. 神经网络模型**

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-010103%402x.png" width="80%">

### 2.2 挑战：因果关系（Casuality）

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-010600%402x.png" width="80%">

### 2.3 挑战：人的问题

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-010720%402x.png" width="45%">

## 3. 可解释 AI 方法的特性

#### 3.1 全局 vs. 局部

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-010901%402x.png" width="60%">

### 3.2 固有（Intrinsic）vs. 事后（post-hoc）

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-011030%402x.png" width="80%">

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-011117%402x.png" width="80%">

<center><font size=2>（来源：Stiglic G, Kocbek S, Pernek I, Kokol P: Comprehensive Decision Tree Models in Bioinformatics. PLoS ONE 7(3): e33812, 2012.）</font></center>

### 3.3 模型不可知（Model-agnostic）vs. 模型特定（model-specific）

**模型特定（Model specific）：**

* 使用模型的内部工作原理和特性来推导出可解释性机制。

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-011459%402x.png" width="35%">

**模型不可知（Model-agnostic）：**

* 仅使用输入和输出来推导出可解释性机制。

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-011513%402x.png" width="35%">

## 4. 可解释 AI 的基本方法

### 4.1 基于归因（Attribution-based）的解释

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-011814%402x.png" width="70%">

<center><font size=2>（来源：T. Miller: "But why?" Understanding explainable artificial i XRDS 25, 3 (Spring 2019), 20–25. <https://doi.org/10.1145/3313107>）</font></center>

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-011920%402x.png" width="80%">

<center><font size=2>（来源：Ribeiro et al.: Why should I trust you?: Explaining the predictions of any classifier. In SIGKDD international conference on knowledge discovery anddata mining. ACM, 2016.）</font></center>

### 4.2 LIME：局部可解释、模型不可知的解释

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-012112%402x.png" width="55%">

<center><font size=2>（来源：Ribeiro et al.: Why should I trust you?: Explaining the predictions of any classifier. In SIGKDD international conference on knowledge discovery anddata mining. ACM, 2016.）</font></center>

### 4.3 基于示例的解释：Prototypes

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-012237%402x.png" width="70%">

<center><font size=2>（来源：Kim et al.: Examples are not enough, learn to criticize! Criticism for interpretability. In NeurIPS. 2016.）</font></center>

### 4.4 基于规则的解释

事后提取规则或直接学习可解释的规则：

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-012418%402x.png" width="70%">

### 4.5 对比解释

> *“关键的洞察力是认识到，一个人并不能解释事件本身，而是一个人解释了为什么令人费解的事件发生在目标案例中，而不是在一些反事实对比案例中。”*
>
> D. J. Hilton, Conversational processes and causal explanation, Psychological Bulletin. 107 (1) (1990) 65–81.

### 4.6 对比解释 —— 差异条件

<img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-22-WX20210623-013124%402x.png" width="80%">

<center><font size=2>（来源：T. Miller. Contrastive Explanation: A Structural-Model Approach, Knowledge Engineering Review, (in print). <https://arxiv.org/abs/1811.03163>）</font></center>

### 4.7 可解释性：总结

**可解释性：**

* 不同的人有不同的可解释性需求
* 信任与伦理
* 人和技术方面的挑战
  * 不透明度
  * 因果关系
  * 人类解释

**可解释性方法：**

* 分类
  * 局部 vs. 全局
  * 可解释 vs. 事后
  * 模型不可知 vs. 模型特定

* 关键方法
  * 归因
  * 示例
  * 规则
  * 对比

## 5. 推荐阅读

* [*Principles and Practice of Explainable Machine Learning.*](http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-08-05-2009.11698.pdf)
* [*“But Why?” Understanding Explainable Artificial Intelligence.*](http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-08-05-but-why-crds-miller.pdf)
* [*Interpretable Machine Learning.*](https://christophm.github.io/interpretable-ml-book/)
