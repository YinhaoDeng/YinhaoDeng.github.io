---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 01：概率论基础   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-04 				# 时间
author:     YEY 						# 作者
header-img: img/post-bg-unimelb.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true                       # 是否启用 MathJax
tags:								#标签
    - 统计机器学习
    - COMP90051
    - 课程笔记
---

# Lecture 01 概率论基础
## 主要内容
* **机器学习：为什么和是什么？**
* **关于 COMP90051**
* **回顾：机器学习基础、概率论**

## 1. 为什么需要机器学习？
* **动机：数据只是信息，而知识是隐藏在数据背后的模式或者模型，我们需要从数据中获取知识。**
  * Data = raw information
  * Knowledge = patterns or models behind the data
* **解决方案：机器学习**
  * 假设：现存的数据仓库中包含许多潜在的有价值的知识。
  * 学习任务：发现这些知识。
  * 学习定义：从任意数据集中，以规则、规律性、模式、约束或模型的形式，（半）自动提取有效、新颖、有用和可理解的知识。

如今，机器学习的应用广泛而深入，例如：
* 在线广告的选择与投放
* 金融、保险、安全等方面的风险管理
* 高频交易
* 医学诊断
* 采矿和自然资源
* 恶意软件分析
* 药物发现
* 搜索引擎

涉及诸多学科：
* 人工智能
* 统计学
* 连续优化
* 数据库
* 信息检索
* 通讯/信息理论
* 信号处理
* 计算机科学理论
* 哲学
* 心理学与神经生物学
...

各行各业的许多公司聘请机器学习专家：
* 数据科学家
* 分析专家
* 商业分析师
* 统计学家
* 软件工程师
* 研究员
...

## 2. 关于本课程
* **课程内容**  
该主题将涵盖来自：统计学习基础、线性模型、非线性基础、核方法、神经网络、贝叶斯学习、概率图形模型（贝叶斯网络、马尔可夫随机场）、聚类分析、降维、正则化和模型选择。
* **高级机器学习：背景要求**
  * 算法与复杂度：
    * Big-O、终止条件
    * 基本数据结构与算法
    * 扎实的代码功底（Python）
  * 数学:
    * 概率论：概率微积分、离散/连续分布、多变量、指数族、贝叶斯规则
    * 线性代数：向量内积和范数、正交基、矩阵运算、逆、特征向量/值
    * 微积分与优化：偏导数、梯度下降、凸性、拉格朗日乘数

## 3. 机器学习基础
###  3.1 相关术语
* **Instance (实例)**: 有关单个实体/对象的度量。
  * _例如：一条贷款申请。_

* **Attribute (属性，又称特征、解释变量)**: 实例的组成部分。
  * _例如：贷款申请人的薪水、家属人数等。_

* **Label (标签，又称响应、因变量)**: 类别、数值等结果。
  * _例如：罚金 vs 还清。_

* **Examples (案例)**: 带标签的实例。
  * _例如：<(100k, 3), “罚金”>_

* **Models (模型):** 发现的属性和 / 或标签之间的关系。

### 3.2 监督 vs 无监督学习

|            |  数据  | 模型作用   |
| --------   | -----  | ----  |
| 监督学习    | 带标签  |   在新的实例上预测标签     |
| 无监督学习 | 不带标签 | 对相关实例进行集群分类；投影到更低的维度；理解属性之间的关系  |

### 3.3  评估（监督学习）
* 问题导向：采用何种评估指标取决于具体问题。
* 典型流程：
  * 选择 _评估指标_，对比标签与预测结果。
  * 获取一个独立的、带标签的 _测试集_。
  * 在测试集上对评估指标进行 “平均”。
* 评估指标
  * 准确度、列联表、精度-召回率、ROC曲线
* 当数据量不足时，采用 _交叉验证_。

## 4. 概率论基础
* **一个概率空间**
  * 集合 $\Omega$ : 所有可能的结果。  
  _例如：掷一次骰子 $\{1,2,3,4,5,6\}$_
  * 集合 $F$ : 事件集合（$\Omega$的子集）。  
  _例如：$\{\phi,\{1\},...,\{6\},\{1,2\},...,\{5,6\},...,\{1,2,3,4,5,6\}\}$_
  * 概率测度 $P: F \to \Bbb{R} $  
  _例如：$P(\phi)=0,P(\{1\})=1/6,P(\{1,2\})=1/3,...$_

* **概率公理**
  1. 对于 $F$ 中的每一个事件 $f$，都有 $P(f)\ge0$
  2. 对于不相交事件对的所有集合，有 $P(U_f f)=\sum_f P(f)$
  3. $P(\Omega)=1$

* **随机变量**
  * 随机变量 $X$ 是结果的一个数值函数，$X(\omega)\in \Bbb{R}$
  * $P(X\in A)$ 表示 $X$ 落在 $A$ 范围内的结果的概率  
  _例如: 赌注为 5 美金, $X$ 表示当掷出偶数时赢钱_  
  _$X$ 将 $1,3,5$ 映射为 $-5$_  
  _$X$ 将 $2,4,6$ 映射为 $5$_  
  _$P(X=5)=P(X=-5)=1/2$_

* **离散分布和连续分布**
  * 离散分布
    * 随机变量取值为离散值
    * 由概率质量函数 $p(x)$ 描述, 即 $P(X=x)$
    * $P(X\le x)=\sum_{a=-\infty}^{x} p(a)$
    * 例如：伯努利分布、二项分布、多项式分布、泊松分布
  * 连续分布
    * 随机变量取值为连续的实数值
    * 由概率密度函数 $p(x)$ 描述
    * $P(X\le x)=\int_{-\infty}^{x} p(a)$
    * 例如：均匀分布、正态分布、拉普拉斯分布、Gamma 分布、Beta 分布、狄利克雷分布

* **期望与方差**
  * 期望 $E[X]$ 是随机变量 $X$ 的 “平均” 值
    * 离散：$E[X]=\sum_x xP(X=x)$
    * 连续：$E[X]=\int_x xP(x)dx$
  * 性质
    * 线性：  
    $E[aX+b]=aE(X)+b$  
    $E[X+Y]=E(X)+E(Y)$
    * 单调性：  
    $X\ge Y \Rightarrow E(X)\ge E(Y)$
  * 方差
  $Var(X)=E[(X-E[X])^2]$

* **独立性与条件概率**
  * $X,Y$ 是互相独立的，如果
    * $P(X\in A,Y\in B)=P(X\in A)P(Y\in B)$
    * 类似地，对于概率密度函数：$p_{X,Y}(x,y)=p_X(x)p_Y(y)$
    * 直观地：知道 $Y$ 的值对于了解 $X$ 没有提供任何信息
    * 代数上：$X,Y$ 的联合概率可以分解成两个因子的乘积
  * 条件概率
    * $P(A\mid B)=\frac{P(A\cap B)}{P(B)}$
    * 类似地，对于概率密度函数：$p(y\mid x)=\frac{p(x,y)}{p(x)}$
    * 直观地：已知事件 $B$ 发生的情况下，事件 $A$ 发生的概率
    * $X,Y$ 独立等价于 $P(Y=y\mid X=x)=P(Y=y)$

* **逆转条件：贝叶斯定理**
  * 对于事件 $A,B$
    * $P(A\cap B)=P(A\mid B)P(B)=P(B\mid A)P(A)$
    * $P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B)}$
  * 通过简单的规则让我们得以交换条件的顺序
  * 贝叶斯统计推断大量使用
    * 边缘概率：单个变量的概率
    * 边缘化：将所有感兴趣的随机变量加起来  
    $P(A)=\sum_b P(A,B=b)$

## 总结
* 为什么需要机器学习？
* COMP90051
* 机器学习基础
* 回顾概率论

下节内容：统计思想流派-有多少种机器学习算法
