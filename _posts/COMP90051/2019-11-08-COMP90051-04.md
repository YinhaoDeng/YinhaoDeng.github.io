---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 04：Logistic 回归与基扩展   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-08 				# 时间
author:     YEY 						# 作者
header-img: img/post-bg-unimelb.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true                       # 是否启用 MathJax
tags:								#标签
    - 统计机器学习
    - COMP90051
    - 课程笔记
---

# Lecture 4 Logistic 回归与基扩展
## 主要内容
* **Logistic 回归**
  * 一种主流的二分类算法
* **基扩展**
  * 通过数据转换扩展模型的表现能力
  * 线性回归和 Logistic 回归的例子
  * 理论注释

## 1. Logistic 回归模型
**一种主流的二分类算法**
### 1.1 二分类：例子

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8da4ba4y8j30ku0mgq43.jpg" width="30%" align="right">

* 例子：给定体重指数（BMI），患者是否患有 2 型糖尿病（T2D）？
* 这种类型的问题称为二分类问题
* 一种方法是采用线性回归
  * 用数据拟合一条直线 / 超平面
  (找到权重 $\boldsymbol w$)
  * 记为 $s\equiv \boldsymbol{x'w}$
  * 如果 $s\ge 0.5$，预测为 “是” 
  * 如果 $s< 0.5$，预测为 “否” 

### 1.2 分类策略

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dc07u76yj30nq0iwtb1.jpg" width="30%" align="right">

* 这种方法容易受到异常值的影响
* 总体而言，最小二乘标准在这种情况下看起来不自然
* 有许多专门针对二分类问题而开发的方法
* 例如，Logistic 回归、感知器、支持向量机（SVM）
<br><br>

### 1.3 Logistic 回归模型

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dcjbutp6j31820okwjl.jpg" width="60%">

* 概率分类方法
  * $P(Y=1\mid\boldsymbol x)=f(\boldsymbol x)=?$
  * 采用线性函数？例如：$s(\boldsymbol x)=\boldsymbol{x'w}$
* 问题是：概率需要介于 0 到 1 之间。
* Logistic 函数：$f(s)=\dfrac{1}{1+\exp(-s)}$
* Logistic 回归模型：

  $P(Y=1 \mid \boldsymbol x)=\dfrac{1}{1+\exp(-\boldsymbol{x'w})}$
* 等价于对数几率比的线性模型：

  $\log \dfrac{P(Y=1 \mid \boldsymbol x)}{P(Y=0 \mid \boldsymbol x)}=\boldsymbol{x'w}$

**思考：Logistic 回归是一种线性方法吗？**

### 1.4 Logistic 回归是一种线性分类器
* Logistic 回归模型：

  $P(Y=1 \mid \boldsymbol x)=\dfrac{1}{1+\exp(-\boldsymbol{x'w})}$
* 分类规则：

  如果 $P(Y=1 \mid \boldsymbol x)>\dfrac{1}{2}$ ，则分类为 “1”，否则分类为 “0”
* 决策边界：

  $\dfrac{1}{1+\exp(-\boldsymbol{x'w})}=\dfrac{1}{2}$ ，即 $\boldsymbol{x'w}=0$

### 1.5 参数向量的影响（二维问题）

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dct0d4vqj30v40k846k.jpg" width="60%">

* 决策边界是直线 $P(Y=1 \mid \boldsymbol x)=0.5$
  * 在高维问题中，决策边界是一个平面或者超平面
* 向量 $\boldsymbol w$ 与决策边界呈直角
  * 即 $\boldsymbol w$ 是决策边界的法线
  * 注意：出于简化考虑，图中我们假设 $w_0=0$

### 1.6 线性 vs. Logistic 概率模型
* 线性回归假设一个正态分布，具有固定的方差和均值，由线性模型给出：

  $p(y \mid \boldsymbol x)=Normal(\boldsymbol{x'w},\sigma^2)$
* Logistic 回归假设一个伯努利分布，参数由线性模型的 Logistic 变换给出：

  $p(y \mid \boldsymbol x)=Bernoulli(\text{logistic}(\boldsymbol{x'w}))$
* 回忆伯努利分布的定义：

  $p(1)=\theta$ 和 $p(0)=1-\theta$，其中 $\theta\in[0,1]$
* 相当于：

  $p(y)=\theta^y(1-\theta)^{1-y}$，其中 $y\in\{0,1\}$

### 1.7 利用最大似然估计训练
* 假设数据点之间互相独立，数据的概率为：

  $p(y_1,...,y_n \mid \boldsymbol x_1,...,\boldsymbol x_n)=\prod_{i=1}^{n}p(y_i \mid \boldsymbol x_i)$
* 假设伯努利分布，可以得到：

  $p(y_i \mid \boldsymbol x_i)=(\theta(\boldsymbol x_i))^{y_i}(1-\theta(\boldsymbol x_i))^{1-y_i}$

  其中，$\theta(\boldsymbol x_i)=\dfrac{1}{1+\exp(-\boldsymbol{x'w})}$
* 训练：最大化上式的权重 $\boldsymbol w$

### 1.8 采用 Log 技巧进行简化
* 相比最大化似然函数，我们选择最大化其对数：

  $$\begin{eqnarray}
  \log\left(\prod_{i=1}^{n}p(y_i \mid \boldsymbol x_i)\right)&=&\sum_{i=1}^{n}\log p(y_i \mid \boldsymbol x_i)\\
  &=& \sum_{i=1}^{n}\log\left((\theta(\boldsymbol x_i))^{y_i}(1-\theta(\boldsymbol x_i))^{1-y_i}\right)\\
  &=& \sum_{i=1}^{n}\left(y_i\log(\theta(\boldsymbol x_i))+(1-y_i)\log(1-\theta(\boldsymbol x_i))\right)\\
  &=& \sum_{i=1}^{n}\left((y_i-1)\boldsymbol x_i'\boldsymbol w-\log(1+\exp(-\boldsymbol x_i'\boldsymbol w))\right)\\
  \end{eqnarray}$$

### 1.9 迭代优化

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8de125bwpj30hc0h6dmw.jpg" width="30%" align="right">

* 训练 Logistic 回归相当于寻找使对数似然函数最大化的 $\boldsymbol w$
* 解析方法：将目标函数的偏导数设为零，求解 $\boldsymbol w$
* **坏消息**：没有闭合解，必须采用迭代方法（例如：梯度下降、牛顿-拉弗森、迭代加权最小二乘）
* **好消息**：如果没有不相关特征，问题是严格凸的（碗形），可以保证优化方法是有效的

展望（Lecture 5）：正则化可以帮助处理不相关特征

## 2. Logistic 回归：决策理论视角
**其中损失函数就是交叉熵**
### 2.1 交叉熵
* 交叉熵是一种比较两个分布的方法
* 交叉熵是对参考分布 $g_{ref}(a)$ 和估计分布 $g_{est}(a)$ 之间散度的一种度量。
对于离散分布：

  $H(g_{ref},g_{est})=-\sum\limits_{a\in A}g_{ref}(a)\log g_{est}(a)$

  其中，$A$ 是分布的支撑集，例如：$A=\{0,1\}$

### 2.2 训练使得交叉熵最小化
* 考虑一个单独数据点的对数似然函数：

  $\log p(y_i \mid \boldsymbol x_i)=y_i\log(\theta(\boldsymbol x_i))+(1-y_i)\log(1-\theta(\boldsymbol x_i))$
* 上式为负交叉熵
* 交叉熵：

  $H(g_{ref},g_{est})=-\sum_a g_{ref}(a)\log g_{est}(a)$
* 参考（真实）分布为：

  $g_{ref}(1)=y_i$ 和 $g_{ref}(0)=1-y_i$
* Logistic 回归将该分布估计为：

  $g_{ref}(1)=\theta(\boldsymbol x_i)$ 和 $g_{ref}(0)=1-\theta(\boldsymbol x_i)$

  它找到使得每个训练数据交叉熵的和最小化的 $\boldsymbol w$

## 3. 基扩展
**通过数据转换扩展模型的可用性**
### 3.1 线性回归的基扩展

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8df2ee2ylj30cc0emq3e.jpg" width="20%" align="right">

* 让我们后退一步。回到线性回归和最小二乘
* 实际数据可能是非线性的
* 如果我们仍然想使用线性回归，应该怎么办？
  * 因为它简单、易于理解、计算效率高等。
* 如何将非线性数据与线性方法结合起来？

_如果你无法击败他们，那就加入他们。_

### 3.2 转换数据
* 诀窍是转换数据：将数据映射到另一个特征空间，在该空间内数据是线性的
* 将该转换记为 $\varphi:\Bbb R^m \rightarrow \Bbb R^k$

  如果 $\boldsymbol x$ 是原始特征集合，那么 $\varphi(\boldsymbol x)$ 表示新的特征集合
* 例子：假设只有一个特征 $x$，数据点呈一条抛物线而非直线<br>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dfek4hy1j30d407uwej.jpg" width="20%">

### 3.3 例子：多项式回归
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dfngo8myj30cm098aab.jpg" width="20%" align="right">

* 别担心，我们可以定义：

  $\varphi_1(x)=x$

  $\varphi_2(x)=x^2$
* 接下来，对 $\varphi_1$ 和 $\varphi_2$ 应用线性回归：

  $y=w_0+w_1\varphi_1(x)+w_2\varphi_2(x)=w_0+w_1x+w_2x^2$

  现在，我们得到了二次回归
* 更一般地，如果新的属性集合是 $x$ 的幂，我们可以得到多项式回归

### 3.4 基扩展
* 数据转换，也称为基扩展，是一种通用技术
  * 在整个课程中，我们将看到更多的例子
* 它既可以用于回归问题，也可以用于分类问题
* $\varphi$ 有很多可能的选择

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dfrc1rdgj30uu0duwfy.jpg" width="50%">

### 3.5 Logistic 回归的基扩展
* 二分类问题例子：数据集不是线性可分的
* 定义转换：

  $\varphi_i(\boldsymbol x)=\\|\boldsymbol x - \boldsymbol z_i\\|$ ，其中 $\boldsymbol z_i$ 是某些预定义的常量
* 选择 $\boldsymbol z_1=[0,0]',\boldsymbol z_2=[0,1]',\boldsymbol z_3=[1,0]',\boldsymbol z_4=[1,1]'$

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dfy38turj319y0h641r.jpg" width="80%">

### 3.6 径向基函数
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8dgcpagn4j30h40e8402.jpg" width="30%" align="right">

* 上面的转换是使用径向基函数（RBF）的一个例子
  * 它们的使用来自近似理论，其中 RBF 的和用于对给定函数近似
* 径向基函数的形式是：

  $\varphi(\boldsymbol x)=\psi(\\|\boldsymbol x-\boldsymbol z\\|)$ ，其中 $\boldsymbol z$ 是一个常量
* 例子：
  * $\varphi(\boldsymbol x)=\\|\boldsymbol x - \boldsymbol z\\|$
  * $\varphi(\boldsymbol x)=\exp\left(-\dfrac{1}{\sigma}\\|\boldsymbol x - \boldsymbol z\\|^2\right)$

### 3.7 基扩展的挑战
* 基扩展可以大大提高方法的可用性，尤其是线性方法
* 在上面的例子中，一个局限是转换需要预先定义：
  * 需要选择新的数据集的尺度
  * 如果使用 RBF，需要选择 $\boldsymbol z_i$
* 对于 $\boldsymbol z_i$，可以选择等距点，或者对训练数据进行聚类并使用聚类中心
* 另一种常见做法是使用训练数据 $\boldsymbol z_i\equiv\boldsymbol x_i$
  * 例如： $\varphi_i(\boldsymbol x)=\psi(\\|\boldsymbol x - \boldsymbol x_i\\|)$
  * 然而，对于很大的数据集，这会导致特征数量巨大，使得计算困难

### 3.8 接下来的方向
* 有几种途径可以将基扩展的思想带入一个新的层次
  * 在后面的学习中将会讨论
* 一种想法是从数据中 _学习_ 转换 $\varphi$
  * 例如：人工神经网络
* 另一种强大的扩展是使用核方法
  * 例如：核感知器
* 最后，在稀疏核机中，训练仅仅依赖于一些少量的数据点
  * 例如：SVM

## 总结
* Logistic 回归
  * 主流的线性分类器
* 基扩展
  * 通过数据转换扩展模型的表现能力
  * 线性回归和 Logistic 回归的相关例子
  * 理论解释

下节内容：正则化避免过拟合、病态优化；相关算法例子
