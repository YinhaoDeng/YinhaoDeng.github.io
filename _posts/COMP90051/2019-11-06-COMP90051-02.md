---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 02：统计思想流派   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-06 				# 时间
author:     YEY 						# 作者
header-img: img/post-bg-unimelb.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true                       # 是否启用 MathJax

tags:								#标签
    - 统计机器学习
    - COMP90051
    - 课程笔记
---

# Lecture 2 统计思想流派
## 主要内容
* **学习算法是如何产生的？**
  * 频率统计学
  * 统计决策理论
  * 贝叶斯统计学
* **概率模型的类型**
  * 参数模型 vs. 非参数模型
  * 生成式模型 vs. 判别式模型

## 1. 频率统计学
**其中未知的模型参数被视为具有固定但未知的值。**

### 1.1 频率统计学
* 抽象问题
  * 已知：$X_1,X_2,...,X_n$ 来自某未知的独立同分布（i.i.d.）
  * 目标：识别该未知分布，或者其性质
* 参数方法（“参数估计”）
  * 模型 $\\{p_\theta(x):\theta \in \Theta\\}$ 的类别由参数 $\Theta$ （可以是实数或者向量等）索引
  * 点估计 $\hat \theta (x_1,x_2,...,x_n)$ 是数据的一个函数（或者统计量）
* 例子
  * 抛 $n$ 次硬币，求正面朝上的概率
  * 选择一种分类器

### 1.2 偏差、方差和渐近变体
频率学家在理想条件下寻求好的表现
* 偏差：$B_\theta(\hat\theta)=E_\theta[\hat\theta(X_1,...,X_n)]-\theta$
* 方差：$Var_\theta(\hat\theta)=E_\theta[(\hat\theta - E_\theta[\hat\theta])^2]$

渐近性质
* 相合性：当 $n \rightarrow \infty$ 时，$\hat\theta(X_1,...,X_n)$ 收敛于 $\theta$
* 有效性：渐近方差尽可能小

### 1.3 最大似然估计（MLE）
* 一个设计估计量的一般原则
* 涉及优化
* $\hat\theta(x_1,...,x_n) \in \mathop{\operatorname{arg\,max}}\limits_{\theta\in\Theta} \prod_{i=1}^{n}p_\theta(x_i)$ （乘积是因为假设数据点之间互相独立）
* MLE 估计量是相合的（在一定条件下）

#### 例子 1：伯努利分布
* 已知数据来自参数未知的伯努利分布（例如偏向硬币），找出该分布的均值
* 均值的 MLE 估计
  * $p_\theta(x)=\begin{cases}\theta,\quad \text{if }x=0\\\\1-\theta,\quad \text{if }x=1\end{cases}=\theta^x (1-\theta)^{1-x}$
  (注意：对于所有其他的 $x$，$p_\theta(x)=0$)
  * 最大化似然函数得到：$\hat\theta=\dfrac{1}{n}\sum_{i=1}^n X_i$

#### 例子 2：正态分布
* 已知数据来自方差为 1 但均值未知的正态分布，找出该分布的均值
* 均值的 MLE 估计
  * $p_\theta(x)=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}(x-\theta)^2)$
  * 最大化似然函数得到：$\hat\theta=\dfrac{1}{n}\sum_{i=1}^n X_i$
* 练习：证明当方差为 $\sigma^2$ 时，MLE 似然函数为 $p_\theta(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$，这里 $\theta=(\mu,\sigma^2)$

#### MLE “算法”
1. 给定数据 $X_1,...,X_n$ 定义概率分布 $p_\theta$，假设数据是由该分布生成的
2. 写出数据的似然函数 $\prod_{i=1}^n p_\theta(X_i)$ （为了方便计算，通常取其对数）
3. 通过优化找到最优（使得数据可能性最大）的参数 $\hat\theta$
    * 将对数似然函数对 $\theta$ 求偏导；令其等于 0，求解
    * 如果不行，使用迭代梯度法

## 2. 统计决策理论
**是统计学、优化理论、经济学、控制理论的分支，强调效用最大化。**

### 2.1 决策理论
* 采取行动以最大化效用 - 与经济学和运筹学相关
* 决策规则 $\delta(\boldsymbol x) \in A$，其中 $A$ 表示行动空间
  * 例如：点估计 $\hat\theta(x_1,...,x_n)$
  * 例如：样本外预测 $\hat Y_{n+1}\mid X_1,Y_1,...,X_n,Y_n,X_{n+1}$
* 损失函数 $l(a,\theta)$：经济成本、误差度量
  * 例如：平方损失估计 $(\hat\theta - \theta)^2$
  * 例如：分类器预测结果的 0-1 损失 $1[y\ne \hat y]$

### 2.2 风险与经验风险最小化（ERM）
* 在决策理论中，我们真正关心的是期望损失
* 损失 $R_\theta[\delta]=E_{\boldsymbol{X}\sim \theta}[l(\delta(\boldsymbol{X}),\theta)]$
  * 例如：真实测试误差，或者泛化误差
* 目标：选择 $\delta$ 使得 $R_\theta[\delta]$ 最小
* 无法直接完成，为什么？
* ERM：使用训练集 $\boldsymbol{X}$ 来近似估计 $p_\theta$
  * 经验风险最小化 $\hat R_\theta[\delta]=\frac{1}{n}\sum_{i=1}^n l(\delta(X_i),\theta)$

### 2.3 展望 Lecture 3
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8cxn7949mj30n40m2add.jpg" width="50%">

* 优化与机器学习
  * 最大似然估计
  * 经验风险最小化
  * ...

### 2.4 偏差-方差分解
* 偏差：$B_\theta(\hat\theta)=E_\theta[\hat\theta(X_1,...,X_n)]-\theta$
* 方差：$Var_\theta(\hat\theta)=E_\theta[(\hat\theta - E_\theta[\hat\theta])^2]$
* 平方损失风险的偏差-方差分解
  $E_\theta[(\theta - \hat \theta)^2]=[B(\hat \theta)]^2+Var_\theta (\hat \theta)$

## 3. 贝叶斯统计学
**其中未知的模型参数具有反映先验信念的关联分布。**

### 3.1 贝叶斯统计学
* 概率对应信念
* 参数
  * 对随机变量的分布建模
  * 对 $\theta$ 的先验信念由先验分布 $P(\theta)$ 编码
    * 同随机变量类似，对参数建模（即使其并非真的随机）
    * 因此，数据的似然函数 $P_\theta(X)$ 写成条件概率的形式 $P(X\mid \theta)$
  * 不同于点估计 $\hat\theta$，贝叶斯会结合观测数据，将先验分布 $P(\theta)$ 更新为后验分布 $P(\theta\mid X)$

### 3.2 概率推断工具
* 贝叶斯统计推断
  * 初始条件给出先验分布 $P(\theta)$ 和似然函数 $P(X\mid \theta)$
  * 观测数据 $X=x$
  * 将先验更新为后验 $P(\theta\mid X=x)$
* 获得后验的基本工具（通用的概率工具，并不局限于贝叶斯统计或者机器学习）
  * 贝叶斯规则：逆转条件的顺序

    $P(\theta\mid X=x)=\dfrac{P(X=x\mid \theta)P(\theta)}{P(X=x)}$
  * 边缘化：消除不必要的变量

    $P(X=x)=\sum_t P(X=x,\theta=t)$ （这被称为证据）

#### 例子
* 我们对 $X\mid\theta$ 建模为 $N(\theta,1)$，先验为 $N(0,1)$
* 假设我们观测到 $X=1$，然后更新先验<br>

  $$\begin{aligned}
  P(\theta|X=1) &= \dfrac{P(X=1| \theta)P(\theta)}{P(X=1)} \quad\quad\color{purple}{\text{目标是将后验转换为已知分布形式。指数的二次方一定为正态}}\\
  &\propto P(X=1| \theta)P(\theta) \\
  &=\left[\color{purple}{\dfrac{1}{\sqrt{2\pi}}}\exp\left(-\dfrac{(1-\theta)^2}{2}\right)\right]\left[\color{purple}{\dfrac{1}{\sqrt{2\pi}}}\exp\left(-\frac{\theta^2}{2}\right)\right] \quad\quad\color{purple}{\text{丢弃关于 }\theta\text{ 的常数项}}\\
  &\propto \exp\left(-\dfrac{(1-\theta)^2+\theta^2}{2}\right) \quad\quad\color{purple}{\text{合并指数项}}\\
  &= \exp\left(-\dfrac{2\theta^2-2\theta+1}{2}\right) \\
  &= \exp\left(-\dfrac{\theta^2-\theta+\frac{1}{2}}{2\times \color{purple}{\frac{1}{2}}}\right) \quad\quad\color{purple}{\text{将分子项中 }\theta^2\text{ 的系数移到分母上}}\\
  &= \exp\left(-\dfrac{\theta^2-\theta+\color{purple}{\frac{1}{4}}}{2\times \frac{1}{2}}\right) \cdot \color{purple}{\exp\left(-\dfrac{\frac{1}{4}}{2\times \frac{1}{2}}\right)}  \quad\quad\color{purple}{\text{将分子项凑成平方形式：移除多余的常数项}}\\
  &\propto \exp\left(-\dfrac{\theta^2-\theta+\frac{1}{4}}{2\times \frac{1}{2}}\right)\\
  &= \exp\left(-\dfrac{(\theta-\frac{1}{2})^2}{2\times \frac{1}{2}}\right)  \quad\quad\color{purple}{\text{因式分解}}\\
  &\propto N(0.5,0.5) \quad\quad\quad\color{purple}{\text{发现为（非标准）正态分布}}
  \end{aligned}$$

  注意：允许将常量提到前面，并通过归一化 “忽略”

### 3.3 如何利用贝叶斯进行点估计
* 通常我们不这么做，除非被拿枪顶着脑袋。。。
  * 因为后验已经包含了全部信息，为什么要丢弃掉呢？
* 但是，还是有一些通用的方法
  * 后验均值： $E_{\theta\mid X}[\theta]=\int \theta P(\theta\mid X)d\theta$
  * 后验模式： $\mathop{\operatorname{arg\,max}}\limits_\theta P(\theta\mid X)$ (最大化后验概率 MAP)
  * 这些是贝叶斯决策理论的解释 
  <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8cvty5gc4j30li0b4my8.jpg" width="50%">

### 3.4 贝叶斯上下文中的 MLE
* MLE 公式：寻找对数据拟合最好的参数
  $\hat\theta\in\mathop{\operatorname{arg\,max}}\limits_\theta P(X=x\mid \theta)$
* 在贝叶斯公式下，考虑 MAP<br>

  $$\begin{aligned}
  \hat\theta &\in \mathop{\operatorname{arg\,max}}\limits_\theta P(\theta\mid X=x)\\
             &= \mathop{\operatorname{arg\,max}}\limits_\theta \dfrac{P(X=x\mid \theta)P(\theta)}{P(X=x)} \\
             &= \mathop{\operatorname{arg\,max}}\limits_\theta P(X=x\mid \theta)P(\theta)
  \end{aligned}$$
  
* 可以看到，和 MLE 相比，MAP 多了一项权重，即先验 $P(\theta)$
* MLE 可以看作 MAP 先验是均匀分布的情况，即 $P(\theta) \propto 1$

### 3.5 频率学派 vs. 贝叶斯学派
* 统计思想的两个主要流派
  * 决策理论对两者都有补充
* 过去：两者存在争议甚至敌意; 几乎像是对于 “宗教” 的选择
* 如今：两者紧密联系

## 4. 概率模型的类别
### 4.1 参数模型 vs. 非参数模型

参数模型 | 非参数模型
------- | --------
由固定的、有限数量的参数确定 | 参数数量随着数据的增长而增加，可能有无限多
缺乏灵活性 | 更加灵活
统计和计算方面更高效 | 效率较低
例如：Logistic 回归、感知机、朴素贝叶斯、简单的神经网络等|例如：KNN、决策树、SVM 等

频率学派和贝叶斯学派都有参数/非参数模型。

### 4.2 生成式模型 vs. 判别式模型
* $X$ 是实例，$Y$ 是标签 （监督学习设置）
  * 给定：来自独立同分布的数据 $(X_1,Y_1),...,(X_n,Y_n)$
  * 发现模型，可以在新的 $X$ 上预测 $Y$
* 生成式方法
  * 对完整的联合概率建模 $P(X,Y)$
* 判别式方法
  * 仅对条件概率建模 $P(Y\mid X)$
* 两者各有优劣

频率学派和贝叶斯学派都有生成式/判别式模型。

## 总结
* 哲学角度：频率学派 vs. 贝叶斯学派
* 许多机器学习模型背后的原理：
  * MLE
  * 风险最小化
  * 概率推断、MAP
* 参数模型 vs. 非参数模型
* 生成式模型 vs. 判别式模型

下节内容：线性回归与优化（需要 MLE，ERM 等）


