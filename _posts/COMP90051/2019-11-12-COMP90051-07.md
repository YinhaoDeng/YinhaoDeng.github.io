---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 07：多层感知器和反向传播   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-12 				# 时间
author:     YEY 						# 作者
header-img: img/post-bg-unimelb.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true                       # 是否启用 MathJax
tags:								#标签
    - 统计机器学习
    - COMP90051
    - 课程笔记
---

# Lecture 07 多层感知器和反向传播
## 主要内容
* **多层感知器**
  * 模型结构
  * 通用近似定理
  * 预训练
* **反向传播**
  * 按步推导
  * 正则化注意事项

## 1. 多层感知器
**通过复合函数对非线性建模**
### 动物园里的动物
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jwez95q2j30vk0k4taf.jpg" width="60%">

* 递归神经网络不在本节范围内
* 自动编码器一种是经过特定方式训练的人工神经网络（ANN）。
  * 例如，一个多层感知器或者递归神经网络可以被训练为一个自动编码器。

### 1.1 线性模型的局限
有些问题是线性可分的，但还有很多不是。

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8jwumawd9j30uu0923z7.jpg" width="80%">

可能的解：复合  
$x_1 \text{ XOR } x_2=(x_1 \text{ OR } x_2)\text{ AND not }(x_1 \text{ AND } x_2)$  
我们将组合一个感知器

### 1.2 感知器是人工神经网络（ANN）的基础
* ANN 不仅限于二分类
* ANN 中的节点可以具有不同的激活函数
  * Step 函数：$f(s)=\begin{cases}1, \quad \text{if }s\ge 0\\\\0, \quad \text{if }s<0\end{cases}$
  <br>
  * Sign 函数：$f(s)=\begin{cases}1, \quad \text{if }s\ge 0\\\\-1, \quad \text{if }s<0\end{cases}$
  <br>
  * Logistic 函数：$f(s)=\dfrac{1}{1+e^{-s}}$
  <br>

  还有很多其他的：tanh 函数、rectifier 函数 等

### 1.3 前馈人工神经网络
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k0bc3qapj31320oah1r.jpg" width="80%">

### 1.4 ANN 作为复合函数
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k0suy6bcj317e0nkwut.jpg" width="85%">

### 1.5 监督学习中的 ANN
* ANN 可以自然地适应各种监督学习设置，例如单变量和多变量回归，以及二分类和多分类问题。
* 单变量回归：$y=f(\boldsymbol x)$
  * 例如：之前学过的线性回归
* 多变量回归：$\boldsymbol y=f(\boldsymbol x)$
  * 预测多个连续结果的值
* 二分类
  * 例如：预测一位病人是否患有 II 型糖尿病
* 多分类
  * 例如：手写数字识别，标签为 $“0”,“1”,...,“9”$ 等
  
### 1.6 ANN 作为非线性模型的能力
* ANN 具有近似各种非线性函数的能力，例如：$z(x)=x^2$ 和 $z(x)=\sin(x)$
* 例如，考虑如下网络。在这个例子中，隐藏单元激活函数是 tanh <br>
  <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k39tzu5aj31c80hg4li.jpg"> 
  
  <br>
  <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8k3e5e1cej313s0jsdk7.jpg" width="80%">
  
  * 蓝色点是在不同 $x$ 处评估的函数值
  * 红色线是来自 ANN 的预测
  * 虚线是隐藏单元的输出
* **通用近似定理**（Cybenko 1989）：一个具有隐藏层、有限数量单元、以及对激活函数适当假设的 ANN，可以很好地近似 $\boldsymbol R^n$ 的紧子集上的连续函数。<br>  
  ( 注意：该定理只是表明理论上存在一个可以拟合任何连续函数的神经网络，但并不代表我们的训练算法一定能够找到这样一个网络，它描述的仅仅是模型本身的特性，与训练过程无关。此外，这并非神经网络的独有特性，也存在其他可以近似任意连续函数的模型，例如，选择了适当的核函数的 SVM )

## 2. 如何训练你的神经网络？
* 你应该知道这个过程：定义损失函数并找到参数，使得在训练数据上的损失函数最小化。
* 接下来，我们将使用**批次大小**为 1 的**随机梯度下降**。也就是说，我们将逐一处理训练样本。

### 2.1 训练设置：单变量回归
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krc5x553j30hk0iqtas.jpg" width="30%" align="right">

* 考虑回归
* 此外，我们采用具有唯一输出的激活函数：  
  $z=h(s)=s=\sum_{j=0}^{p}u_jw_j$
* 这将简化反向传播的描述。在其他设置下，训练过程是类似的。

**思考：** 右图所示的 ANN 中一共有多少个参数？假设存在偏置节点 $x_0$ 和 $u_0$ 的情况下。

对于隐藏层，一共有 $p(m+1)$ 个输入参数，$p+1$ 个输出参数。  
所以，该网络一共有 $p(m+1)+(p+1)=(m+2)p+1$ 个参数。

### 2.2 ANN 训练中的损失函数
* 在训练样本 $\\{\boldsymbol x,y\\}$ 和预测 $\hat f(\boldsymbol x,\boldsymbol \theta)=z$ 之间需要 **损失函数**，其中，$\boldsymbol \theta$ 是由 $v_{ij}$ 和 $w_j$ 组成的参数向量。
* 对于回归问题，可以采用 **平方和误差**  
  $L=\dfrac{1}{2}\left(\hat f(\boldsymbol x,\boldsymbol \theta)-y\right)^2=\dfrac{1}{2}(z-y)^2$  
  ( 前面的常数项是为了数学计算上的方便 )
* **决策理论** 训练：最小化 $L \text{ w.r.t. }\boldsymbol \theta$
  * 幸运的是，$L(\boldsymbol \theta)$ 是可微的
  * 不幸的是，通常情况下不存在解析解

### 2.3 ANN 中的随机梯度下降
* 选择初始的 $\boldsymbol \theta^{(0)}$ 和 $k=0$  
  ( 这里，$\boldsymbol \theta$ 是一个所有层的所有权重的集合 )
* 从 $i=1$ 到 $T$（轮）：
  * 从 $j=1$ 到 $N$（训练样本）：
    * 考虑样本 $\\{\boldsymbol x_j,y_j\\}$
    * 更新：$\boldsymbol \theta^{(i+1)}=\boldsymbol \theta^{(i)}-\eta\nabla L(\boldsymbol \theta^{(i)})$  
      其中，  
      $L=\dfrac{1}{2}(z_j-y_j)^2$  
      需要计算偏导数 $\dfrac{\partial L}{\partial v_{ij}}$ 和 $\dfrac{\partial L}{\partial w_j}$

## 3. 反向传播
**= “误差的反向传播”**
**计算一个复合函数的损失函数的梯度**
### 3.1 反向传播：从链式法则出发
* 回忆一个 ANN 的输出 $z$ 是一个复合函数，因此，$L(z)$ 也是一个复合函数<br>  
  $$\begin{eqnarray}
  L &=& 0.5(z-y)^2=0.5(h(s)-y)^2=0.5(s-y)^2\\
  &=& 0.5\left( \sum_{j=0}^{p}u_jw_j-y \right)^2 = 0.5\left(\sum_{j=0}^{p}g(r_j)w_j-y\right)^2=...
  \end{eqnarray}$$

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8krc5x553j30hk0iqtas.jpg" width="30%" align="right">

* 反向传播利用了链式求导法则
  <br>  
  * $\dfrac{\partial L}{\partial w_j}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial w_j}$
  <br>
  * $\dfrac{\partial L}{\partial v_{ij}}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial u_j}\dfrac{\partial u_j}{\partial r_j}\dfrac{\partial r_j}{\partial v_{ij}}$

### 3.2 反向传播：中间步骤
* 应用链式法则：
  <br>  
  * $\dfrac{\partial L}{\partial w_j}=\color{red}{\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}}\dfrac{\partial s}{\partial w_j}$
  <br>
  * $\dfrac{\partial L}{\partial v_{ij}}=\color{red}{\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial u_j}\dfrac{\partial u_j}{\partial r_j}}\dfrac{\partial r_j}{\partial v_{ij}}$
  <br>
* 现在，我们定义：
  <br>
  * $\delta\equiv\dfrac{\partial L}{\partial s}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}$
  <br>
  * $\varepsilon_j\equiv\dfrac{\partial L}{\partial r_j}=\dfrac{\partial L}{\partial z}\dfrac{\partial z}{\partial s}\dfrac{\partial s}{\partial u_j}\dfrac{\partial u_j}{\partial r_j}$
  <br>
* 这里，$L=0.5(z-y)^2$ 并且 $z=s$<br>  
  因此，$\color{red}{\delta=(z-y)}$
  <br>
* 这里，$s=\sum_{j=0}^{p}u_jw_j$ 并且 $u_j=g(r_j)$<br>
  因此，$\color{red}{\varepsilon_j=\delta w_jg'(r_j)}$

### 3.3 反向传播等式
* 我们有
  <br>
  * $\dfrac{\partial L}{\partial w_j}=\delta \color{red}{\dfrac{\partial s}{\partial w_j}}$ ， 其中，$\delta=\dfrac{\partial L}{\partial s}=(z-y)$
  <br>
  * $\dfrac{\partial L}{\partial v_{ij}}=\varepsilon_j \color{red}{\dfrac{\partial r_j}{\partial v_{ij}}}$ ， 其中，$\varepsilon_j=\dfrac{\partial L}{\partial r_j}=\delta w_jg'(r_j)$
  <br>
* 回忆，$s=\sum_{j=0}^{p}u_jw_j$ ， $r_j=\sum_{i=0}^{m}x_iv_{ij}$
  <br>
* 因此，$\dfrac{\partial s}{\partial w_j}=u_j$ ， $\dfrac{\partial r_j}{\partial v_{ij}}=x_i$
  <br>
* 我们有
  <br>
  * $\dfrac{\partial L}{\partial w_j}=\delta u_j=(z-y)u_j$
  <br>
  * $\dfrac{\partial L}{\partial v_{ij}}=\varepsilon_j x_i=\delta w_jg'(r_j)x_i$

### 3.4 向前传播
* 用当前估计的 $v_{ij}$ 和 $w_j$ $\quad\Longrightarrow\quad$ 计算 $r_j,u_j,s$ 和 $z$

### 3.5 误差的反向传播
* $\dfrac{\partial L}{\partial v_{ij}}=\varepsilon_jx_i\quad\Longleftarrow\quad\varepsilon_j=\delta w_jg'(r_j)\quad\Longleftarrow\quad\dfrac{\partial L}{\partial w_j}=\delta u_j\quad\Longleftarrow\quad\delta=(z-y)$

### 3.6 ANN 训练中的其他注意事项
* ANN 非常灵活（回忆通用近似定理），但另一方面导致了过度参数化，因此倾向于 **过拟合**
* 起始权重通常随机分布在零附近
* 隐式正则化：**提前停止**
  * 对于有些激活函数，将会使 ANN 退化为线性模型（为什么？）  
  <img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l205se3uj30io0d0jtd.jpg" width="50%">
  如上图所示，假如我们采用 tanh 函数作为激活函数，如果权重接近 0，那么 tanh 函数起作用的部分近似线性，因此，神经网络退化成近似线性模型。

### 3.7 显式正则化
* 或者，我们也可以采用一种显式正则化的方法，这与岭回归中的正则化非常类似
* 相比最小化损失函数 $L$，我们选择最小化正则化后的损失函数：  
  $L+\lambda(\sum_{i=0}^{m}\sum_{j=1}^{p}v_{ij}^2+\sum_{j=0}^{p}w_j^2)$
  <br>  
* 这将在计算偏导数时，简单地加上 $2\lambda v_{ij}$ 和 $2\lambda w_j$ 项
* 对于某些激活函数，这种方法同样会存在 ANN 退化成线性模型的问题

## 总结
* 多层感知器
  * 模型结构
  * 通用近似定理
  * 预训练
* 反向传播
  * 按步推导
  * 正则化注意事项

下节内容：DNN（深度神经网络）、CNN（卷积神经网络）、自动编码器
