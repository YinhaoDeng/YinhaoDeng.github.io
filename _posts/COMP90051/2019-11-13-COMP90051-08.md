---
layout:     post   				    # 使用的布局（不需要改）
title:      统计机器学习 08：深度学习、卷积神经网络和自动编码器   	# 标题 
subtitle:   墨尔本大学 COMP90051 课程笔记 #副标题
date:       2019-11-13 				# 时间
author:     YEY 						# 作者
header-img: img/post-bg-unimelb.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
mathjax: true                       # 是否启用 MathJax
tags:								#标签
    - 统计机器学习
    - COMP90051
    - 课程笔记
---

# Lecture 08 深度学习、卷积神经网络和自动编码器
## 主要内容
* **深度学习**
  * 表示能力
  * 深度模型与表示学习
* **卷积神经网络**
  * 卷积操作
  * 基于卷积的网络的元素
* **自动编码器**
  * 学习有效的编码

## 1. 深度学习与表示学习
**隐藏层被视为特征空间的转换**
### 1.1 表示能力
* 具有单个隐藏层的 ANN 是一个通用近似器
* 例如，这样的 ANN 可以表示任何布尔函数：<br>  
  $OR(x_1,x_2)\qquad \qquad u=g(x_1+x_2-0.5)$  
  $AND(x_1,x_2)\qquad \quad \; u=g(x_1+x_2-1.5)$  
  $NOT(x_1)\qquad \qquad\;\; u=g(-x_1)$  
  如果 $r\ge 0$，那么$g(r)=1$；否则，$g(r)=0$  
  <br>  
* 任何具有 $m$ 个变量的布尔函数都可以由一个最多包含 $2^m$ 个元素的隐藏层实现
* 更有效的做法是将几个隐藏层堆叠起来

### 1.2 深度网络
所谓 “深度” 是指隐藏层的数量。

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l4gj9d27j30x20mq7a3.jpg" width="65%">

$\boldsymbol s=\text{tanh}(\boldsymbol A'\boldsymbol x)\quad\boldsymbol t=\text{tanh}(\boldsymbol B'\boldsymbol s)\quad\boldsymbol u=\text{tanh}(\boldsymbol C'\boldsymbol t)\quad\boldsymbol z=\text{tanh}(\boldsymbol D'\boldsymbol u)$

### 1.3 深度 ANN 作为表示学习
* 连续层构成了越来越复杂的输入的表示
* ANN 可以具有简单的线性输出层，但可以使用复杂的非线性来表示<br>  
  $\boldsymbol z=\text{tanh}\left(\boldsymbol D'\left(\text{tanh}\left(\boldsymbol C'\left(\text{tanh}\left(\boldsymbol B'\left(\text{tanh}(\boldsymbol A'\boldsymbol x)\right)\right)\right)\right)\right)\right)$  
  <br>  
* 等效地，一个隐藏层可以被视为转换后的特征空间，例如：$\boldsymbol u=\varphi(\boldsymbol x)$
* 这种转换的参数是从数据中学习到的

### 1.4 ANN 的层作为数据转换
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l50cd9iej30za0o679o.jpg" width="65%"><br>

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l50bdag5j30za0o8wjy.jpg" width="65%"><br>

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l50apnhtj30za0o80y7.jpg" width="65%"><br>

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8l50a1to1j30za0o843z.jpg" width="65%">

### 1.5 深度 vs. 宽度
* 理论上，一个宽度无限的单层给出了一个通用近似器
* 然而，（从经验上看）深度可以产生更精确的模型  
  从眼睛上获得的生物学灵感：
  * 首先检测小边缘和色块
  * 将它们组合成更小的形状
  * 建立更复杂的检测器，例如 纹理，面孔等
* 试图模仿网络中的分层复杂性
* 但是 **梯度消失问题** 会影响非常深层模型的学习

## 2. 卷积神经网络（CNN）
**基于将小型滤波器重复应用于 2D 图像的小块或者 1D 输入的范围**
### 2.1 卷积
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8lyv6ikk7j313e0q2jtu.jpg" width="65%"><br>

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8lyykvfxbj313g0q20w5.jpg" width="65%">

### 2.2 在 2D 图像上的卷积
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mbcwpu3qj31180qy417.jpg" width="65%">

### 2.3 滤波器作为特征检测器
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mc019a6xj31a40t6dip.jpg" width="65%"><br>

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mc03me07j31ac0t6dis.jpg" width="65%">

### 2.4 堆叠卷积
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mc3jdmjdj31b40pgdhc.jpg" width="65%">

* 开发不同规模和复杂度的复杂表示
* 滤波器是从训练数据中学习的

### 2.5 CNN 用于计算机视觉
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mc8kerw5j31880tuagz.jpg" width="65%">

Implemented by Jizhizi Li based on LeNet5: <http://deeplearning.net/tutorial/lenet.html>

### 2.6 CNN 的组成部分
* 卷积层
  * 基于卷积运算的复杂输入表示
  * 滤波器权重是从训练数据中学到的
* 下采样，通常通过最大池化
  * 重新缩放至较小的分辨率，限制了参数爆炸
* 全连接部件和输出层
  * 合并表示

### 2.7 通过最大池化进行下采样
* 特殊类型的处理层。对于一个 $m\times m$ 的块：  
  $v=\max(u_{11},u_{12},...,u_{mm})$
* 严格来说，并非处处可微的。相反，梯度是根据 “子梯度” 定义的。
  * 非最大的 $u_{ij}$ 值的微小变化不会改变 $v$
  * 如果 $u_{ij}$ 是最大值，其值的微小变化将线性地改变 $v$
  * 如果 $u_{ij}=v$，那么 $\frac{\partial v}{\partial u_{ij}}=1$，否则，$\frac{\partial v}{\partial u_{ij}}=0$
* 正向传播记录最大化元素，然后在反向传播期间将其用于反向传递

### 2.8 卷积作为正则项
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfhzcnmjj311y0p4aet.jpg" width="65%">

## 3. 自动编码器
**一个 ANN 训练设置，可用于无监督学习、初始化，或者仅用于高效编码**
### 3.1 自动编码的思想
* 监督学习：
  * 单变量回归：根据 $\boldsymbol x$，预测 $y$
  * 多变量回归：根据 $\boldsymbol x$，预测 $\boldsymbol y$
* 无监督学习：
  * 探索数据 $\boldsymbol x_1,...,\boldsymbol x_n$
  * 没有响应变量
* 对于每个 $\boldsymbol x_i$，设置 $\boldsymbol y_i\equiv\boldsymbol x_i$
* 训练一个 ANN，根据 $\boldsymbol x_i$ 预测 $\boldsymbol y_i$
* 没有意义吗？

### 3.2 自动编码器的拓扑结构
* 给定一些无标签数据 $\boldsymbol x_1,...,\boldsymbol x_n$，设置 $\boldsymbol y_i\equiv\boldsymbol x_i$，并且训练一个 ANN 来预测 $\boldsymbol z(\boldsymbol x_i)\approx\boldsymbol x_i$
* 设置 **瓶颈层** $\boldsymbol u$，使得中间比输入层 “窄”<br>
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8mftu0m2wj30sk0jywkj.jpg" width="65%">

### 3.3 瓶颈层介绍
* 假设你试图训练一个可以很好地恢复原始信号的网络 $\boldsymbol z(\boldsymbol x_i)\approx\boldsymbol x_i$
* 这意味着数据结构可以用一个较低维度的表示来有效地描述（编码）

### 3.4 降维
* 自动编码器可通过一个非线性变换来达到 **压缩** 和 **降维** 的目的
* 如果你使用线性激活函数，并且仅使用一个隐藏层，则设置几乎变成了 **主成分分析** 的设置（敬请期待！）
  * ANN 可能会找到一个其他的解，不（直接）使用特征值

## 4. 工具
* Tensorflow, Theano, Torch
  * Python / Lua 中用于深度学习的工具链
  * 符号微分或者自动微分
  * GPU 支持快速编译
  * Theano 教程：<http://deeplearning.net/tutorial/>
* 其他
  * Caffe
  * CNTK
  * deeplearning4j...
* Keras：高级 Python API，可以运行在 Tensorflow、CNTK 或者 Theano 上

## 总结
* 深度学习
  * 表示能力
  * 深度模型与表示学习
* 卷积神经网络
  * 卷积运算
  * 一个基于卷积的网络的要素
* 自动编码器
  * 学习高效编码

下节内容：支持向量机（SVM）
